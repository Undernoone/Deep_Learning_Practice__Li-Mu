## AlexNet

AlexNet和LeNet的架构非常相似。 但AlexNet比相对较小的LeNet要深得多。

AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。

AlexNet使用ReLU而不是Sigmoid作为其激活函数。

在这里提供的是一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。

## Code

```python
import torch
from d2l.torch import evaluate_accuracy_gpu
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3,stride=2),
    nn.Conv2d(96,256,kernel_size=5,padding=2),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3,stride=2),
    nn.Conv2d(256,384,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Conv2d(384,384,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Conv2d(384,256,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3,stride=2),
    nn.Flatten(),
    nn.Linear(6400,4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096,4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096,10)) # 由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000

batch_size = 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,resize=224)

lr, num_epochs = 0.01, 10
d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())
d2l.plt.show()
```