# Date Operation

## Data structures

### Dimensionality of data structures 

#### 0 Dimensions (Scalars)

#### 1 Dimension (Vectors)

#### 2 Dimensions (Matrices)

#### 3 Dimensions (RGB image)

```python
#例如一张2x2像素的RGB图片
image = [
    [[255, 0, 0], [0, 255, 0]],   # 第一行
    [[0, 0, 255], [255, 255, 255]] # 第二行
    ]
'''
image[0][0] 是第1行第1列的像素，颜色为红色 [255, 0, 0]
image[0][1] 是第1行第2列的像素，颜色为绿色 [0, 255, 0]
image[1][0] 是第2行第1列的像素，颜色为蓝色 [0, 0, 255]
image[1][1] 是第2行第2列的像素，颜色为白色 [255, 255, 255]
'''
```

#### 4 Dimensions (RGB images in batches)

```python
import numpy as np
# 初始化一个5张100x100像素的RGB图片的批量，所有像素初始值为0（黑色）
batch_size = 5
height = 100
width = 100
channels = 3
batch_images = np.zeros((batch_size, height, width, channels), dtype=np.uint8)
# 设置第1张图片的第2行第3列的像素颜色为RGB(210, 201, 202)
batch_images[0][1][2] = [210, 201, 202]
'''这种四维数组表示法在机器学习和深度学习中非常常见，尤其是在处理图像数据时。例如，在使用卷积神经网络（CNN）训练时，图像数据通常会以这样的四维数组批量传递给模型。'''
```

#### 5 Dimensions (Videos in batches)

```python
import numpy as np
# 初始化一个3个视频，每个视频10帧，每帧64x64像素的RGB视频批量，所有像素初始值为0（黑色）
batch_size = 3
frames = 10
height = 64
width = 64
channels = 3
batch_videos = np.zeros((batch_size, frames, height, width, channels), dtype=np.uint8)
# 设置第1个视频的第2帧第3行第4列的像素颜色为RGB(210, 201, 202)
batch_videos[0][1][2][3] = [210, 201, 202]
'''这种五维数组表示法在视频处理、训练和预测任务中非常常见，尤其是在处理视频数据的深度学习模型中。视频的批量处理可以提高训练和推理的效率，尤其是在处理大规模视频数据集时。'''

```

## Data operations

```python
import torch
import numpy
x = torch.arange(12)
print('x:',x)
print('x.shape:',x.shape)
print(x.numel()) # returns the number of elements in the tensor(int)
x_reshape = x.reshape(3, 4)
print(x_reshape)
print(x_reshape.shape)
print(x.numel()) # returns the number of elements in the tensor(int)
all_zeros = torch.zeros((2,3,4)) # 2 dimension 3 row 4 col tensor with all zeros
print(all_zeros)
all_ones = torch.ones((2,3,4)) # 2 dimension 3 row 4 col tensor with all ones
print(all_ones)
a = torch.tensor([1,2,3],)
b = torch.tensor([4,5,6])
print(a+b) # [5, 7, 9]
print(torch.exp(a)) # [2.71828175, 7.38905621, 20.08553692]
c = torch.arange(12, dtype=torch.float32,).reshape(3,4)
d = torch.arange(12, 24, dtype=torch.float32,).reshape(3,4) # 左闭右开
print(c) 
print(d)
print(torch.cat([c,d],dim=0)) # 维度0表示按行拼接，1表示按列拼接
print(torch.cat([c,d],dim=1)) 
demo = [[1,2,3],[4,5,6]] # List of lists
demo_numpy = numpy.array(demo)  # Convert list of lists to numpy array
demo_tensor = torch.tensor(demo) # Convert list of lists to tensor
print(demo_numpy)
print(demo_tensor)
e = torch.tensor([3.5])
print(e)
print(e.item()) # return the value of tensor as a Python scalar
print(type(e))
print(type(e.item())) # return the type of the scalar value
```

## Data preprocessing

### Create dataset

```python
# 创建一个人工数据集，并存储在csv(字符分隔符文件)
import os
os.makedirs(os.path.join("..", "Deeplearning_Li Mu_Date"), exist_ok=True) # 创建文件夹
data_file = os.path.join("..", "Deeplearning_Li Mu_Date", "data.csv") # 文件路径
with open(data_file, "w") as f:
    f.write('NumRooms,Alley,Price\n')
    f.write('NA,Pave,127500\n')
    f.write('2,NA,106000\n')
    f.write('3,NA,178100\n')
    f.write('4,NA,140000\n')
'''
with open(data_file, "w") as f:是Python中的一种用于文件操作的语法结构，通常被称为上下文管理器(Context Manager)。
with 语句：引入上下文管理器，确保在退出代码块时，文件资源能够被正确释放。
open(data_file, "w")：打开或创建文件 data_file。
data_file是文件路径。
"w"是模式字符串，表示写入模式(write mode)。
"w"模式会覆盖文件中的现有内容。如果文件不存在，会创建一个新文件。
其他常见的模式包括 "r"(读取模式)、"a"(追加模式)、"b"(二进制模式)等。
as f：将文件对象(由open函数返回)赋给变量 f，用于在代码块中引用这个文件对象。
'''
```

### xxxxxxxxxx import torchfrom torch import nnfrom d2l import torch as d2l​# Attention Mechanism# Nadaraya-Watson kernel​# Generate datan_train = 50x_train, _ = torch.sort(torch.rand(n_train) * 5) # ,_ 抛弃第二个返回值即索引​# Target functiondef f(x):    return 2 * torch.sin(x) + x ** 0.8​# 添加噪声y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))​# Test datax_test = torch.arange(0, 5, 0.1)y_truth = f(x_test)x_test_len = len(x_test)print(x_test_len)​def plot_kernel_reg(y_hat):    d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth','Pred'],             xlim=[0,5], ylim=[-1,5])    d2l.plt.plot(x_train, y_train, 'o', alpha=0.5)​# 平均预测结果# maan计算平均值y_hat = torch.repeat_interleave(y_train.mean(), x_test_len)plot_kernel_reg(y_hat)d2l.plt.show()​# 非参数注意力汇聚，沐神将Pooling不叫池化叫汇聚X_test_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))attention_weights = nn.functional.softmax(-(X_test_repeat - x_train) ** 2 / 2, dim=1)y_hat = torch.matmul(attention_weights, y_train)plot_kernel_reg(y_hat)d2l.plt.show()​# 可视化注意力权重d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0),                  xlabel='Sorted training inputs', ylabel='Sorted test inputs')d2l.plt.show()​​weights = torch.ones((2,10)) * 0.1values = torch.arange(20.0).reshape((2,10))print(torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1)))​# 带参数的注意力汇聚class NWKernelRegression(nn.Module):    def __init__(self, **kwargs):        super().__init__(**kwargs)        self.w = nn.Parameter(torch.rand((1,),requires_grad=True))​    def forward(self, queries, keys, values):        queries = queries.repeat_interleave(keys.shape[1]).reshape(-1,keys.shape[1])        self.attention_weights = nn.functional.softmax(-((queries - keys) * self.w)**2/2,dim=1)        return torch.bmm(self.attention_weights.unsqueeze(1),values.unsqueeze(-1)).reshape(-1)X_tile = x_train.repeat((n_train, 1))Y_tile = y_train.repeat((n_train, 1))keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train,-1))values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape(n_train, -1)​net = NWKernelRegression()loss = nn.MSELoss(reduction='none')trainer = torch.optim.SGD(net.parameters(), lr=0.5)animator = d2l.Animator(xlabel='epoch',ylabel='loss',xlim=[1,5])​for epoch in range(5):    trainer.zero_grad()    l = loss(net(x_train, keys, values), y_train) / 2    l.sum().backward()    trainer.step()    print(f'epoch {epoch+1}, loss {float(l.sum()):.6f}')    animator.add(epoch+1, float(l.sum()))d2l.plt.show()​keys = x_train.repeat((x_test_len, 1))values = y_train.repeat((x_test_len, 1))y_hat = net(x_test, keys, values).unsqueeze(1).detach()plot_kernel_reg(y_hat)d2l.plt.show()d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0),                  xlabel='Sorted training inputs', ylabel='Sorted testing inputs')d2l.plt.show()python

```python
import pandas as pd
import os
'''创造数据集'''
data_file = os.path.join('.','01_Data','01_house_tiny.csv')
data = pd.read_csv(data_file)
print(data)
'''读取一个CSV文件，并将其内容加载到一个Pandas DataFrame(data)中，然后打印出DataFrame的内容。'''
```

### Preprocess dataset

```python
import pandas as pd
import os
'''创造数据集+加载数据集'''
inputs, outputs = data.iloc[:,0:2], data.iloc[:,2] # # inputs取前两列，outputs取第三列
inputs = inputs.fillna(inputs.mean()) # 对 NaN 值用均值插值
print(inputs)
# 如有BUG，可尝试以下代码
# inputs['NumRooms'] = pd.to_numeric(inputs['NumRooms'], errors='coerce')
# numeric_columns = inputs.select_dtypes(include=[np.number]).columns
# inputs[numeric_columns] = inputs[numeric_columns].fillna(inputs[numeric_columns].mean())
# print(inputs)
```
### Convert dataset to tensor
```python
import pandas as pd
import os
import torch
'''创造数据集+加载数据集+预处理数据集'''
inputs_tensor, outputs_tensor = torch.tensor(inputs.values), torch.tensor(outputs.values)
print(inputs_tensor)
print(outputs_tensor)
```
