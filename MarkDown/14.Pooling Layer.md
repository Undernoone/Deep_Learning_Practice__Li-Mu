## Pooling Layer

降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

## Max Pooling & Average Pooling

池运算是确定性的，通常计算汇聚窗口中所有元素的最大值或平均值。

在这两种情况下，与互相关运算符一样，汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。

在汇聚窗口到达的每个位置，计算该窗口中输入子张量的最大值或平均值。

## xxxxxxxxxx import osimport torchfrom torch import nnfrom torch.optim import lr_schedulerfrom torchvision import datasets, transformsfrom torch.utils.data import DataLoaderfrom net import LeNet​# Define the transforms for the datadate_transforms = transforms.Compose([    transforms.ToTensor(),])​# Load the datasettrain_dataset = datasets.MNIST('../Deeplearing_data', train=True, transform=date_transforms, download=False)train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)test_dataset = datasets.MNIST('../Deeplearing_data', train=False, transform=date_transforms, download=False)test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)​# If you want to use GPU, uncomment the following linedevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')​# Define the modelmodel = LeNet().to(device)​# Define the loss function and optimizerloss_func = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)​# Define the learning rate schedulerscheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)​# Train the modeldef train(dataloader, model, loss_func, optimizer, device):    model.train()    total_loss, total_correct, total_samples = 0.0, 0, 0    for batch_idx, (data, target) in enumerate(dataloader):        data, target = data.to(device), target.to(device)        output = model(data)        loss = loss_func(output, target)        optimizer.zero_grad()        loss.backward()        optimizer.step()        total_loss += loss.item() * data.size(0)        _, pred = torch.max(output, 1)        total_correct += (pred == target).sum().item()        total_samples += data.size(0)    average_loss = total_loss / total_samples    accuracy = total_correct / total_samples    print(f"train_loss: {average_loss:.4f}")    print(f"train_acc: {accuracy:.4f}")​# Test the modeldef test(dataloader, model, loss_func, device):    model.eval()    total_loss, total_correct, total_samples = 0.0, 0, 0    with torch.no_grad():        for batch_idx, (data, target) in enumerate(dataloader):            data, target = data.to(device), target.to(device)            output = model(data)            loss = loss_func(output, target)            total_loss += loss.item() * data.size(0)            _, pred = torch.max(output, 1)            total_correct += (pred == target).sum().item()            total_samples += data.size(0)    average_loss = total_loss / total_samples    accuracy = total_correct / total_samples    print(f"test_loss: {average_loss:.4f}")    print(f"test_acc: {accuracy:.4f}")    return accuracy​# Start trainingepochs = 50min_acc = 0for epoch in range(epochs):    print(f'epoch {epoch+1}\n----------------')    train(train_dataloader, model, loss_func, optimizer, device)    accuracy = test(test_dataloader, model, loss_func, device)    scheduler.step()  # Adjust learning rate    if accuracy > min_acc:        folder = 'saved_model'        if not os.path.exists(folder):            os.mkdir(folder)        min_acc = accuracy        print('Saving best model')        torch.save(model.state_dict(), 'saved_model/best_model.pth')print('Training complete')​python

```python
import torch
from torch import nn
from d2l import torch as d2l

def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]): # Loop over rows
        for j in range(Y.shape[1]): # Loop over columns
            if mode == 'max':
                Y[i,j] = X[i:i + p_h, j:j + p_w].max()
            elif mode == 'avg':
                Y[i,j] = X[i:i + p_h, j:j + p_w].mean()
    return Y

X = torch.tensor([[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]])
print(pool2d(X, (2,2))) # max
print(pool2d(X, (2,2)，'avg')) # avg
```

## Pytorch Pooling Code

```python
import torch
from torch import nn
from d2l import torch as d2l
X = torch.arange(16,dtype=torch.float32).reshape((1,1,4,4)) 
print(X)
pool2d = nn.MaxPool2d(3) 
# Pytorch的池化层padding默认等于size，stride默认为0
pool2d(X)

pool2d = nn.MaxPool2d(3,padding=1,stride=2)
print(pool2d(X))

pool2d = nn.MaxPool2d((2,3),padding=(1,1),stride=(2,3))
print(pool2d(X))

X = torch.cat((X,X+1),1)
print(X.shape)
print(X)

pool2d = nn.MaxPool2d(3,padding=1,stride=2)
print(pool2d(X))
```

