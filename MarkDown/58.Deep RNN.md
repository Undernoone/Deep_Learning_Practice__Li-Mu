# Deep RNN

字面意思，Deep RNN 是在 RNN 基础上通过堆叠多个隐藏层构建。

例如处理文本：“我喜欢吃苹果。” 

**普通 RNN** 只能理解到“喜欢”和“吃”之间的关系。

**深度 RNN** 通过多层结构，能够理解到“我喜欢吃”，更好地把握“苹果”在句子中的作用。

## Deep RNN Code

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

# 使用PyTorch的LSTM模型不再自己实现，直接调用库即可
# 区别：layers默认为1，这里设置为2
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)

num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)
d2l.plt.show()
```

xxxxxxxxxx import torchfrom torch import nnfrom d2l import torch as d2l​batch_size, num_steps, device = 32, 35, d2l.try_gpu()train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)vocab_size, num_hiddens, num_layers = len(vocab), 256, 2num_inputs = vocab_size# bidirectional是双向参数,只有这一个参数不同lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)model = d2l.RNNModel(lstm_layer, len(vocab))model = model.to(device)num_epochs, lr = 500, 1d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)d2l.plt.show()python
