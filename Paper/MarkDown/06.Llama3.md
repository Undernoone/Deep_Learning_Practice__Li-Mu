# Llama3.1

## Abstract

介绍自己参数：Max版本拥有405B参数、最长支持128k上下文。

再和行业标杆比较：Llama能媲美甚至有些领域优于GPT-4。

**注意：**1b = 1亿 1B = 10亿

## Introduction

介绍换页现代AI模型分为两派：预训练和后训练

pre-training：GPT

post-training：DeepMind、Claude、ChatGPT

**注意：**

GPT和ChatGPT是两个产品

GPT主要用于文本生成、文本翻译......

ChatGPT可以理解成一个基于GPT模型的专业优化对话系统，主要使用了RLHF进行微调

Llama认为LLM的三个关键是：数据、规模和复杂性管理（这个是扯淡的）

**Date**：预训练就是用了15T（15万亿）个tokens，而Llama只使用了1.8T。（写论文一定要对比）

预处理数据集很精细，保证了质量和相关性。后训练筛选也十分严格，提高模型可靠性。

**大模型的数据是十分关键的。**

**Scale**：Llama3使用了$3.8\cdot10^{25}$次浮点运算，进一步提升了学习能力和性能。

**Managing complexity**：因为复杂算法（Reinforcement Learning）训练失败所以美其名曰说Llama为了保证稳定性而使用了简单可靠的算法（Dense Transformer）。

总结自己的模型MAX版本可以媲美GPT-4，且自己的多模态扩展正在开发中（画饼）。

## General Overview

介绍预训练、后训练、多模态视觉训练、语音训练。

## Pre-Training

### Pre-Training Date

去除了PII(personally identifiable information)和adult content

但是不是全去除，只去除了一部分

爬虫方法

去除了HTML中的header和footer只保留content

code只保留换行

说md是有害的？markdown全删除掉了（？？？）因为是纯文本训练，肯定是试验发现去掉结果更好

使用了MiniHash

### Determining the Data Mix

### Annealing Data退火数据（微调）