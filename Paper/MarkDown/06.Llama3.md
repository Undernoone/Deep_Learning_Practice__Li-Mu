# Llama3.1

## Abstract

介绍自己参数：Max版本拥有405B参数、最长支持128k上下文。

再和行业标杆比较：Llama能媲美甚至有些领域优于GPT-4。

**注意：**1b = 1亿 1B = 10亿

## Introduction

介绍换页现代AI模型分为两派：预训练和后训练

pre-training：GPT

post-training：DeepMind、Claude、ChatGPT

**注意：**

GPT和ChatGPT是两个产品

GPT主要用于文本生成、文本翻译......

ChatGPT可以理解成一个基于GPT模型的专业优化对话系统，主要使用了RLHF进行微调

Llama认为LLM的三个关键是：数据、规模和复杂性管理（这个是扯淡的）

**Date**：预训练就是用了15T（15万亿）个tokens，而Llama只使用了1.8T。（写论文一定要对比）

预处理数据集很精细，保证了质量和相关性。后训练筛选也十分严格，提高模型可靠性。

**大模型的数据是十分关键的。**

**Scale**：Llama3使用了$3.8\cdot10^{25}$次浮点运算，进一步提升了学习能力和性能。

**Managing complexity**：因为复杂算法（Reinforcement Learning）训练失败所以美其名曰说Llama为了保证稳定性而使用了简单可靠的算法（Dense Transformer）。

总结自己的模型MAX版本可以媲美GPT-4，且自己的多模态扩展正在开发中（画饼）。

## General Overview

介绍预训练、后训练、多模态视觉训练、语音训练。

## Pre-Training

介绍四个关键指标：训练数据的筛选、模型架构的开发、预训练技术、预训练参数。

### Pre-Training Date

数据截止到2023年底。为了获得高质量tokens，去除了PII(personally identifiable information)和adult content。但是不是全去除，只去除了一部分，因为经过测试adult content效果也不错（产业链发达）。

### Web Data Curation

**爬虫**：

- HTML去除冗余部分（header和footer），只保留content和alt（数学内容常用图片表示)。
- code只保留换行，没保存缩进。
- Markdown全部删除，因为有害？？？因为是纯文本训练，应该是试验后发现去掉md结果更好。

**去重**：

- URL重复只保留最新网页。
- 文档重复使用MinHash算法进行去重。
- 行重复使用类似ccNet的算法删除超过6次的行数据。（删除了广告、导航栏，但同时也损失了一部分性能）。

**启发式过滤**：

去除重复内容，去除成人网站，去除有大量异常token的网站。

**基于模型的质量过滤**：

通过模型进行质量分类，将高质量的tokens权重加大

**代码与推理数据**：

借鉴DeepSeek对于数学和代码使用了专属管道进行训练。

**多语言处理**：

使用Fasttext将文档分类成176种语言，再对每种语言进行去重，且有特殊启发式规则进行筛选过滤。

### Determining the Data Mix

评测网页质量

### Annealing Data退火数据（微调）