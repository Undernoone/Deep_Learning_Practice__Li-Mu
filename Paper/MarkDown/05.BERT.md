# BERT

ELMO使用了双向信息，但是网络架构太老了是RNN，我不是预测未来，我是完型填空

GPT使用了新的Transformer，但是是单向

所以我使用了双向的Transformer

## Input

### NSP二分类任务

处理两个句子之间的关系

所以需要分隔符告诉电脑这是两个符号

CLS

Token Embedding

Seg Embedding

Pos Embedding

## Abstract

BERT全名为Bidirectional Encoder Representations from Transformers。

与最近（2018）的模型不同，BERT使用的是未标注文本的深度双向考虑上下文训练模式。

因此，BERT只需要添加一个额外输出层进行微调。随实现简单但证实强大。

## Introduction

常用的预训练方法有

基于特征的方法如ELMo：通过增加预训练的语言表示来辅助特定任务。

微调方法如GPT：调整预训练好的模型参数来适应下游任务。

这两种方法都不太好，尤其是是微调方法，因为它使用的是单向语言模型，只能从左往右处理信息，对于双向需要上下文理解的任务处理不好，这种限制对于文本模型来说是灾难性的。

所以我们提出了BERT，BERT改进了基于微调的方法，它使用了掩码语言模型缓解了单向语言模型的限制。

什么是掩码语言模型呢？掩码语言模型就是随机掩盖住一些输入，让机器依据上下文去预测被掩盖的部分（下文会详细讲解实现方法）。

## Related Work

讲解NLP的发展历史和常用技术

### BERT

预训练，微调